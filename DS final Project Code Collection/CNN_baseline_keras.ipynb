{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn\n",
    "import yaml\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shubhamadep/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "with open(\"config.yml\", 'r') as ymlfile:\n",
    "    cfg = yaml.load(ymlfile)\n",
    "dataset_name = \"mrpolarity\"\n",
    "datasets = data_helpers.get_datasets_mrpolarity(cfg[\"datasets\"][dataset_name][\"positive_data_file\"][\"path\"],\n",
    "                                                    cfg[\"datasets\"][dataset_name][\"negative_data_file\"][\"path\"])\n",
    "x_text, y = data_helpers.load_data_labels(datasets)\n",
    "y_target = datasets['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-861b4fe95a2d>:2: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /Users/shubhamadep/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /Users/shubhamadep/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "[[   1    2    3 ...    0    0    0]\n",
      " [   8  173   19 ...    0    0    0]\n",
      " [ 267   26  268 ...    0    0    0]\n",
      " ...\n",
      " [  10  956   12 ...    0    0    0]\n",
      " [   1   87  162 ...    0    0    0]\n",
      " [   8  929 2110 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 9683\n",
      "Train/Dev split: 1024/320\n",
      "input shape :  (1024, 834)\n"
     ]
    }
   ],
   "source": [
    "# Split train/test set\n",
    "# TODO: This is very crude, should use cross-validation\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, stratify = y)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.2, stratify = y_train)\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_test)))\n",
    "print('input shape : ', x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_vectors_word2vec(vocabulary, filename, binary):\n",
    "    encoding = 'utf-8'\n",
    "    count = 0;\n",
    "    with open(filename, \"rb\") as f:\n",
    "        header = f.readline()\n",
    "        vocab_size, vector_size = map(int, header.split())\n",
    "        # initial matrix with random uniform\n",
    "        embedding_vectors = np.random.uniform(-0.25, 0.25, (len(vocabulary), vector_size))\n",
    "        if binary:\n",
    "            binary_len = np.dtype('float32').itemsize * vector_size\n",
    "            for line_no in range(vocab_size):\n",
    "                word = []\n",
    "                while True:\n",
    "                    ch = f.read(1)\n",
    "                    if ch == b' ':\n",
    "                        break\n",
    "                    if ch == b'':\n",
    "                        raise EOFError(\"unexpected end of input; is count incorrect or file otherwise damaged?\")\n",
    "                    if ch != b'\\n':\n",
    "                        word.append(ch)\n",
    "                word = str(b''.join(word), encoding=encoding, errors='strict')\n",
    "                idx = vocabulary.get(word)\n",
    "                #if word in top_words:\n",
    "                if idx != 0:\n",
    "                    count += 1\n",
    "                    warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "                    embedding_vectors[idx] = np.fromstring(f.read(binary_len), dtype='float32')\n",
    "                else:\n",
    "                    f.seek(binary_len, 1)\n",
    "        else:\n",
    "            for line_no in range(vocab_size):\n",
    "                line = f.readline()\n",
    "                if line == b'':\n",
    "                    raise EOFError(\"unexpected end of input; is count incorrect or file otherwise damaged?\")\n",
    "                parts = str(line.rstrip(), encoding=encoding, errors='strict').split(\" \")\n",
    "                if len(parts) != vector_size + 1:\n",
    "                    raise ValueError(\"invalid vector on line %s (is this really the text format?)\" % (line_no))\n",
    "                word, vector = parts[0], list(map('float32', parts[1:]))\n",
    "                idx = vocabulary.get(word)\n",
    "                if idx != 0:\n",
    "                    embedding_vectors[idx] = vector\n",
    "        f.close()\n",
    "        return embedding_vectors\n",
    "\n",
    "def load_embedding_vectors_glove(vocabulary, filename, vector_size):\n",
    "    # load embedding_vectors from the glove\n",
    "    # initial matrix with random uniform\n",
    "    embedding_vectors = np.random.uniform(-0.25, 0.25, (len(vocabulary), vector_size))\n",
    "    f = open(filename)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "        idx = vocabulary.get(word)\n",
    "        if idx != 0:\n",
    "            embedding_vectors[idx] = vector\n",
    "    f.close()\n",
    "    return embedding_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.06207478 -0.16540766 -0.01926712 ...  0.16078679  0.04741327\n",
      "  -0.16492658]\n",
      " [ 0.08007812  0.10498047  0.04980469 ...  0.00366211  0.04760742\n",
      "  -0.06884766]\n",
      " [ 0.25390625  0.03857422 -0.03039551 ...  0.18554688 -0.15625\n",
      "   0.03881836]\n",
      " ...\n",
      " [-0.22070312 -0.02807617  0.07861328 ...  0.17773438 -0.27734375\n",
      "  -0.0378418 ]\n",
      " [-0.33789062  0.20507812 -0.09765625 ... -0.22363281  0.44921875\n",
      "   0.12402344]\n",
      " [ 0.07470703  0.05102539  0.05712891 ... -0.04614258 -0.21289062\n",
      "  -0.02307129]]\n"
     ]
    }
   ],
   "source": [
    "embedding_file_path = '../GoogleNews-vectors-negative300.bin'\n",
    "vocabulary = vocab_processor.vocabulary_\n",
    "EMBEDDING_DIM = 300\n",
    "embedding_matrix_word2vec = load_embedding_vectors_word2vec(vocabulary, embedding_file_path, EMBEDDING_DIM)\n",
    "print(embedding_matrix_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file_path = 'glove/glove.6B.300d.txt'\n",
    "vocabulary = vocab_processor.vocabulary_\n",
    "EMBEDDING_DIM = 300\n",
    "embedding_matrix_glove = load_embedding_vectors_glove(vocabulary, embedding_file_path, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(X_data, y_data, batch_size):\n",
    "\n",
    "  samples_per_epoch = X_data.shape[0]\n",
    "  number_of_batches = samples_per_epoch/batch_size\n",
    "  counter=0\n",
    "\n",
    "  while 1:\n",
    "\n",
    "    X_batch = np.array(X_data[batch_size*counter:batch_size*(counter+1)]).astype('float32')\n",
    "    y_batch = np.array(y_data[batch_size*counter:batch_size*(counter+1)]).astype('float32')\n",
    "    counter += 1\n",
    "    yield X_batch,y_batch\n",
    "\n",
    "    #restart counter to yeild data in the next epoch as well\n",
    "    if counter >= number_of_batches:\n",
    "        counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = vocab_processor.vocabulary_\n",
    "EMBEDDING_DIM = 300\n",
    "W = np.random.uniform(-0.25, 0.25, len(vocabulary)*EMBEDDING_DIM)\n",
    "W = W.reshape(len(vocabulary), EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = vocab_processor.vocabulary_\n",
    "EMBEDDING_DIM = 300\n",
    "W = np.random.uniform(-1, 1, len(vocabulary)*EMBEDDING_DIM)\n",
    "W = W.reshape(len(vocabulary), EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "values_w2v = []\n",
    "for x in embedding_matrix_word2vec:\n",
    "    for val in x:\n",
    "        values_w2v.append(val)\n",
    "\n",
    "W = np.random.uniform(-1, 1, len(vocabulary)*EMBEDDING_DIM)\n",
    "X = np.random.uniform(-0.5, 0.5, len(vocabulary)*EMBEDDING_DIM)\n",
    "Y = np.random.uniform(-0.25, 0.25, len(vocabulary)*EMBEDDING_DIM)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [9, 5]\n",
    "\n",
    "sns.distplot(values_w2v)\n",
    "sns.distplot(W);\n",
    "sns.distplot(X);\n",
    "sns.distplot(Y);\n",
    "o_patch = mpatches.Patch(color='red', label='weights: +0.25, -0.25')\n",
    "r_patch = mpatches.Patch(color='green', label='weights: +0.50, -0.50')\n",
    "p_patch = mpatches.Patch(color='orange', label='weights: +1, -1')\n",
    "b_patch = mpatches.Patch(color='blue', label='Weights assigned by word2vec')\n",
    "plt.legend(handles=[o_patch, r_patch, p_patch, b_patch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size :  0.95\n",
      "1520/1520 [==============================] - 2s 1ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2322\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2323\u001b[0;31m         \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_OperationGetAttrValueProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2324\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Operation 'loss_9/dense_14_loss/Mean' has no attr named '_XlaCompile'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m       \u001b[0mxla_compile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_XlaCompile\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m       xla_separate_compiled_gradients = op.get_attr(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2326\u001b[0m       \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2327\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2328\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Operation 'loss_9/dense_14_loss/Mean' has no attr named '_XlaCompile'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c92bb99fd0fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m#print(classifier.summary())#, callbacks = callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps_for_each_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_multiprocessing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;31m#         print(history.history[\"val_loss\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdo_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    507\u001b[0m                     training_updates = self.optimizer.get_updates(\n\u001b[1;32m    508\u001b[0m                         \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collected_trainable_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m                         loss=self.total_loss)\n\u001b[0m\u001b[1;32m    510\u001b[0m                 updates = (self.updates +\n\u001b[1;32m    511\u001b[0m                            \u001b[0mtraining_updates\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/optimizers.py\u001b[0m in \u001b[0;36mget_updates\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_updates_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/optimizers.py\u001b[0m in \u001b[0;36mget_gradients\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             raise ValueError('An operation has `None` for gradient. '\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(loss, variables)\u001b[0m\n\u001b[1;32m   2755\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mgradients\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2756\u001b[0m     \"\"\"\n\u001b[0;32m-> 2757\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients)\u001b[0m\n\u001b[1;32m    628\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     return _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops,\n\u001b[0;32m--> 630\u001b[0;31m                             gate_gradients, aggregation_method, stop_gradients)\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, src_graph)\u001b[0m\n\u001b[1;32m    812\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 814\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    815\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    406\u001b[0m       \u001b[0mxla_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_XlaScope\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Exit early\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mxla_compile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    812\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 814\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    815\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MeanGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_MeanGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m   \u001b[0;34m\"\"\"Gradient for Mean.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m   \u001b[0msum_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_SumGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m   \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m   \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_SumGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m   \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m   \u001b[0;31m# TODO(apassos) remove this once device placement for eager ops makes more\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m   \u001b[0;31m# sense.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(input, name, out_type)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m   \"\"\"\n\u001b[0;32m--> 221\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mshape_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape_internal\u001b[0;34m(input, name, optimize, out_type)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moptimize\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(input, out_type, name)\u001b[0m\n\u001b[1;32m   7246\u001b[0m     \u001b[0mout_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"out_type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7247\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 7248\u001b[0;31m         \"Shape\", input=input, out_type=out_type, name=name)\n\u001b[0m\u001b[1;32m   7249\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7250\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    707\u001b[0m                   \u001b[0;34m\"Attr '%s' of '%s' Op passed %d less than minimum %d.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m                   (key, op_type_name, attr_value.i, attr_def.minimum))\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mattr_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"list(int)\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m           \u001b[0mattr_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_MakeInt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mattr_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"float\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "all_accuracies = []\n",
    "all_variance = []\n",
    "#test_sizes = [0.95, 0.90, 0.80, 0.70, 0.60, 0.50, 0.40, 0.30, 0.20, 0.10]\n",
    "test_sizes = [0.95]\n",
    "\n",
    "for test_size in test_sizes:\n",
    "    print('Test size : ',test_size)\n",
    "    accuracies = []\n",
    "    epochs_size = 100\n",
    "    for i in range(0, 5):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = test_size, stratify = y)\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.2, stratify = y_train)\n",
    "\n",
    "        callback = [EarlyStopping(monitor='val_loss',\n",
    "                                      min_delta=0,\n",
    "                                      patience=0,\n",
    "                                      verbose=0, mode='auto')]\n",
    "        batch_size = 32\n",
    "        steps_for_each_epoch = int( np.ceil(x_train.shape[0] / batch_size) )\n",
    "#         print(x_train.shape[0], steps_for_each_epoch)\n",
    "#         sess = tf.Session(config=tf.ConfigProto( allow_soft_placement=True, log_device_placement=True))\n",
    "        classifier = Sequential()\n",
    "        classifier.add(Embedding(len(vocabulary), EMBEDDING_DIM, weights=[embedding_matrix_word2vec], input_length=x_train.shape[1])) #trainable=False)\n",
    "        classifier.add(Conv1D(32, 3, activation = 'relu'))\n",
    "        classifier.add(MaxPooling1D(pool_size = 3))\n",
    "        classifier.add(Flatten())\n",
    "        classifier.add(Dense(units = 16, activation = 'sigmoid', kernel_regularizer = regularizers.l2(0.0001)))\n",
    "        classifier.add(Dropout(0.50))\n",
    "        classifier.add(Dense(units = 2, activation = 'softmax'))\n",
    "        classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "        #print(classifier.summary())#, callbacks = callback\n",
    "        history = classifier.fit_generator(generator(x_train, y_train, batch_size),callbacks = callback, epochs = 20, steps_per_epoch = steps_for_each_epoch, validation_data = (x_val, y_val), validation_steps = 100, shuffle = True, use_multiprocessing = True, verbose = 0)\n",
    "#         print(history.history[\"val_loss\"])\n",
    "        loss, accuracy = classifier.evaluate(x_test, y_test)\n",
    "        #print('Accuracy: %f' % (accuracy*100))\n",
    "        accuracies.append(accuracy)\n",
    "#     print(accuracies)\n",
    "#     print(np.mean(accuracies))\n",
    "#     print(np.var(accuracies))\n",
    "    all_accuracies.append(np.mean(accuracies))\n",
    "    all_variance.append(np.var(accuracies))\n",
    "print('Accuracies : ', all_accuracies)\n",
    "print('Variance : ', all_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1520/1520 [==============================] - 2s 1ms/step\n",
      "0.7948317879124691 0.5\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "filename = 'save_model.pkl'\n",
    "pickle.dump(classifier, open(filename, 'wb'))\n",
    "\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "a, b = loaded_model.evaluate(x_test, y_test)\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracies :  [0.5830263157894737, 0.6704166666666667, 0.67359375, 0.6435714285714286, 0.7758333333333333, 0.83925, 0.8649999999999999, 0.9266666666666667, 0.92875, 0.9237499999999998]\n",
    "Variance :  [0.004138054016620498, 0.00067295524691358, 0.0026590332031249967, 0.005238966836734694, 0.010931163194444446, 0.008273499999999996, 0.010642187500000004, 0.00013611111111111113, 8.359374999999987e-05, 0.00039687499999999933]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with regularization\n",
    "\n",
    "Accuracies :  [0.5621052631578948, 0.686875, 0.8456249999999998, 0.8074999999999999, 0.8838541666666666, 0.850625, 0.9390625, 0.9393750000000001, 0.931875, 0.936875]\n",
    "Variance :  [0.0050677977839335185, 0.011039009452160494, 0.008696557617187501, 0.015737595663265307, 0.008059733072916664, 0.011838203124999998, 5.3222656250000156e-05, 0.00021132812500000006, 0.00041953124999999977, 0.0006128906249999999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-34-4d3b18f54cb4>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-34-4d3b18f54cb4>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    weights : -1, +1\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "weights : -1, +1 \n",
    "[0.7404935956001282, 0.7478448748588562, 0.6508960127830505, 0.6543195247650146, 0.765687495470047, 0.6632035672664642, 0.7020750939846039, 0.7374310493469238, 0.7079029679298401, 0.7221793234348297, 0.7645464539527893, 0.7894353568553925, 0.7979920208454132, 0.8461835384368896, 0.8875622153282166, 0.9119525253772736, 0.9236847162246704, 0.9429888725280762, 0.9228183925151825, 0.9201494455337524]\n",
    "\n",
    "weights : -0.50, +0.50\n",
    "[0.7529853284358978, 0.7550478279590607, 0.6417356133460999, 0.6444882154464722, 0.6766594052314758, 0.6598062813282013, 0.8815954625606537, 0.7431097328662872, 0.7604756951332092, 0.713350772857666, 0.7318406999111176, 0.7911670804023743, 0.829595148563385, 0.8419949412345886, 0.8437898457050323, 0.8505183756351471, 0.8709539771080017, 0.8794790804386139, 0.8905494213104248, 0.8980833292007446]\n",
    "\n",
    "weights : word2vec\n",
    "[0.7263475656509399, 0.7861708700656891, 0.6272746473550797, 0.627626970410347, 0.6100166887044907, 0.5543903410434723, 0.4856374114751816, 0.3910641223192215, 0.32671790570020676, 0.3506401628255844, 0.29860566556453705, 0.2849557548761368, 0.3232623040676117, 0.2825904116034508, 0.2753669172525406, 0.2933805137872696, 0.28422221541404724, 0.2705959901213646, 0.2731320708990097, 0.2936372309923172]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"For partition 5:95, 10 runs. without word2vec\"\n",
    "[0.6256578947368421, 0.5, 0.5, 0.6532894736842105, 0.5, 0.5006578947368421, 0.5, 0.625, 0.6664473684210527, 0.6638157894736842]\n",
    "mean accuracy : 0.5734868421052631\n",
    "variance : 0.005545018178670362\n",
    "\n",
    "with word2vec \n",
    "[0.6131578947368421, 0.6776315789473685, 0.6664473684210527, 0.8, 0.525, 0.6585526315789474, 0.6690789473684211, 0.6552631578947369, 0.6690789473684211, 0.6605263157894737]\n",
    "0.6594736842105263\n",
    "0.004055851800554017\n",
    "\n",
    "with glove\n",
    "\n",
    "[0.5894736842105263, 0.6177631578947368, 0.5572368421052631, 0.6730263157894737, 0.5, 0.5, 0.65, 0.7289473684210527, 0.6381578947368421, 0.5046052631578948]\n",
    "0.595921052631579\n",
    "0.0057105090027700835\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"For partition 10:90, 10 runs.without word2vec\"\n",
    "[0.6173611111111111, 0.6840277777777778, 0.6159722222222223, 0.6611111111111111, 0.5104166666666666, 0.6694444444444444, 0.6416666666666667, 0.6590277777777778, 0.6465277777777778, 0.6340277777777777]\n",
    "mean accuracy : 0.6339583333333334\n",
    "variance : 0.0021210889274691364\n",
    "\n",
    "with word2vec\n",
    "[0.6604166666666667, 0.7902777777777777, 0.6631944444444444, 0.6708333333333333, 0.66875, 0.8597222222222223, 0.6569444444444444, 0.8680555555555556, 0.5895833333333333, 0.5]\n",
    "0.6927777777777777\n",
    "0.012017669753086425\n",
    "\n",
    "with glove \n",
    "[0.6555555555555556, 0.6680555555555555, 0.6923611111111111, 0.6611111111111111, 0.64375, 0.70625, 0.7673611111111112, 0.6326388888888889, 0.6583333333333333, 0.6659722222222222]\n",
    "0.6751388888888888\n",
    "0.0013557870370370386"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"For partition 15:85, 10 runs. without word2vec\"\n",
    "mean accuracy : 0.6258088235294117\n",
    "variance : 0.003372561634948095\n",
    "\n",
    "with word2vec\n",
    "0.7704411764705882\n",
    "0.010178395328719721"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"For partition 20:80, 10 runs. without word2vec\"\n",
    "[0.6703125, 0.66640625, 0.64453125, 0.665625, 0.66796875, 0.66171875, 0.73203125, 0.55703125, 0.6546875, 0.66875]\n",
    "mean accuracy : 0.65890625\n",
    "variance : 0.00034194946289062506\n",
    "\n",
    "with word2vec\n",
    "[0.91640625, 0.81953125, 0.678125, 0.63515625, 0.90078125, 0.6625, 0.87421875, 0.890625, 0.8828125, 0.896875]\n",
    "0.815703125\n",
    "0.011252008056640627\n",
    "\n",
    "with glove\n",
    "[0.7109375, 0.68515625, 0.67109375, 0.60859375, 0.63671875, 0.74609375, 0.884375, 0.6515625, 0.62578125, 0.66484375]\n",
    "0.6885156250000001\n",
    "0.005742559814453126"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"For partition 30:70, 10 runs. without word2vec\"\n",
    "[0.8098214285714286, 0.7901785714285714, 0.6785714285714286, 0.6839285714285714, 0.6839285714285714, 0.64375, 0.6741071428571429, 0.6678571428571428, 0.6616071428571428, 0.8294642857142858]\n",
    "mean accuracy : 0.7123214285714287\n",
    "variance : 0.004275318877551021\n",
    "\n",
    "with word2vec\n",
    "[0.8803571428571428, 0.7607142857142857, 0.8357142857142857, 0.8910714285714286, 0.8723214285714286, 0.8901785714285714, 0.55625, 0.8160714285714286, 0.9178571428571428, 0.8178571428571428]\n",
    "0.8238392857142858\n",
    "0.009913113839285712\n",
    "\n",
    "\n",
    "with glove\n",
    "[0.8839285714285714, 0.6455357142857143, 0.8660714285714286, 0.8276785714285714, 0.8178571428571428, 0.6669642857142857, 0.8767857142857143, 0.6133928571428572, 0.7169642857142857, 0.8571428571428571]\n",
    "0.7772321428571428\n",
    "0.009974689094387753"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"For partition 40:60, 10 runs. without word2vec\"\n",
    "[0.63125, 0.6973214285714285, 0.6357142857142857, 0.6107142857142858, 0.6848214285714286, 0.7348214285714286, 0.66875, 0.6491071428571429, 0.6589285714285714, 0.6598214285714286, 0.7020833333333333, 0.6166666666666667, 0.8208333333333333, 0.6760416666666667, 0.61875, 0.7333333333333333, 0.6739583333333333, 0.7677083333333333, 0.7864583333333334, 0.71875]\n",
    "mean accuracy : 0.6872916666666666\n",
    "variance : 0.003242219387755101\n",
    "\n",
    "with word2vec\n",
    "0.9004166666666666\n",
    "0.0021658420138888903\n",
    "\n",
    "with glove\n",
    "[0.9041666666666667, 0.9010416666666666, 0.8416666666666667, 0.7927083333333333, 0.7583333333333333, 0.8989583333333333, 0.8385416666666666, 0.9083333333333333, 0.8604166666666667, 0.9041666666666667]\n",
    "0.8608333333333332\n",
    "0.0025093750000000003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"For partition 50:50, 10 runs. without word2vec\"\n",
    "[0.74125, 0.79125, 0.74625, 0.625, 0.67375, 0.83125, 0.67125, 0.71875, 0.7575, 0.67625]\n",
    "mean accuracy : 0.72325\n",
    "variance : 0.003535062500000001\n",
    "\n",
    "with word2vec\n",
    "[0.95625, 0.89375, 0.945, 0.88375, 0.94, 0.91375, 0.88875, 0.90375, 0.875, 0.8825]\n",
    "0.90825\n",
    "0.0007672499999999994\n",
    "\n",
    "with glove\n",
    "[0.8125, 0.88375, 0.8525, 0.8975, 0.85375, 0.79875, 0.8275, 0.885, 0.8775, 0.82]\n",
    "0.8508749999999999\n",
    "0.0010828281250000005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"For partition 60:40, 10 runs. without word2vec\"\n",
    "[0.63125, 0.6973214285714285, 0.6357142857142857, 0.6107142857142858, 0.6848214285714286, 0.7348214285714286, 0.66875, 0.6491071428571429, 0.6589285714285714, 0.6598214285714286, 0.7020833333333333, 0.6166666666666667, 0.8208333333333333, 0.6760416666666667, 0.61875, 0.7333333333333333, 0.6739583333333333, 0.7677083333333333, 0.7864583333333334, 0.71875, 0.75875, 0.81625, 0.75875, 0.74, 0.8125, 0.795, 0.67, 0.84625, 0.73, 0.7875, 0.8234375, 0.8359375, 0.8234375, 0.8875, 0.80625, 0.778125, 0.85625, 0.7796875, 0.83125, 0.8234375]\n",
    "mean accuracy : 0.7834375\n",
    "variance : 0.0035870117187500007\n",
    "\n",
    "with word2vec\n",
    "[0.8927083333333333, 0.8864583333333333, 0.9208333333333333, 0.9447916666666667, 0.890625, 0.91875, 0.934375, 0.9229166666666667, 0.9322916666666666, 0.89375]\n",
    "0.9137500000000001\n",
    "0.00040143229166666637\n",
    "\n",
    "with glove\n",
    "[0.909375, 0.9296875, 0.890625, 0.7703125, 0.884375, 0.925, 0.875, 0.8515625, 0.86875, 0.8765625]\n",
    "0.878125\n",
    "0.0018457031250000016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"For partition 70:30, 10 runs. without word2vec\"\n",
    "[0.784375, 0.653125, 0.8140625, 0.840625, 0.803125, 0.803125, 0.7796875, 0.775, 0.8734375, 0.7078125]\n",
    "mean accuracy : 0.8045833333333332\n",
    "variance : 0.003388715277777779\n",
    "\n",
    "with word2vec\n",
    "[0.9475, 0.91, 0.93875, 0.8725, 0.9525, 0.9475, 0.94625, 0.95, 0.9175, 0.93]\n",
    "0.93125\n",
    "0.0005706249999999992\n",
    "\n",
    "with glove\n",
    "[0.9395833333333333, 0.8416666666666667, 0.91875, 0.9, 0.9020833333333333, 0.8958333333333334, 0.9020833333333333, 0.9083333333333333, 0.9125, 0.9041666666666667]\n",
    "0.9025000000000001\n",
    "0.0005527777777777773"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"For partition 75:25, 10 runs. without word2vec\"\n",
    "mean accuracy : 0.827\n",
    "variance : 0.0013559999999999993"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"For partition 80:20, 10 runs. without word2vec\"\n",
    "[0.859375, 0.8375, 0.871875, 0.884375, 0.878125, 0.903125, 0.875, 0.63125, 0.834375, 0.859375]\n",
    "mean accuracy : 0.8434375\n",
    "variance : 0.005389550781250001\n",
    "\n",
    "with word2vec\n",
    "[0.91875, 0.95625, 0.9, 0.9, 0.903125, 0.94375, 0.934375, 0.9625, 0.928125, 0.94375]\n",
    "0.9290624999999999\n",
    "0.0004786132812500003\n",
    "\n",
    "with glove\n",
    "[0.9125, 0.9375, 0.8875, 0.940625, 0.91875, 0.95625, 0.90625, 0.88125, 0.9375, 0.884375]\n",
    "0.91625\n",
    "0.0006285156250000008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"For partition 90:10, 10 runs. without word2vec\"\n",
    "[0.925, 0.8625, 0.86875, 0.84375, 0.8375, 0.9, 0.86875, 0.85, 0.9125, 0.875]\n",
    "mean accuracy : 0.8743749999999999\n",
    "variance : 0.0007769531250000002\n",
    "\n",
    "with word2vec\n",
    "[0.9375, 0.95625, 0.95, 0.91875, 0.94375, 0.925, 0.94375, 0.90625, 0.95, 0.93125]\n",
    "0.93625\n",
    "0.00022499999999999986\n",
    "\n",
    "with glove\n",
    "[0.91875, 0.925, 0.9375, 0.8875, 0.95, 0.9375, 0.95625, 0.91875, 0.95, 0.89375]\n",
    "0.9275\n",
    "0.00049375"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"For partition 90:10, 10 runs. with word2vec\"\n",
    "mean accuracy : 0.9243750000000001\n",
    "variance : 0.0008863281250000002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6616071428571428, 0.7160714285714286, 0.5758928571428571, 0.6964285714285714, 0.6553571428571429, 0.6410714285714286, 0.6366071428571428, 0.5883928571428572, 0.6375, 0.6848214285714286]\n",
      "0.6493749999999999\n",
      "0.0017595742984693884\n"
     ]
    }
   ],
   "source": [
    "print(accuracies)\n",
    "print(np.mean(accuracies))\n",
    "print(np.var(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
